\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[a4paper, portrait, margin=0.75in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[sort,round]{natbib}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{setspace}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage[skip=8pt, indent=0pt]{parskip}
\usepackage{titlesec}

\bibliographystyle{plainnat}

\titlespacing\section{0pt}{12pt}{4pt}

% Title and author information
\title{\textbf{Identifying Key Socioeconomic Determinants of Income Inequality: \\
A Machine Learning Approach Using World Bank Data}}

\author{
    Anonymous Author\\
    \small Department of Economics\\
    \small University Name\\
    \small \texttt{email@university.edu}
}

\date{\today}

% Document settings
\singlespacing

\begin{document}

\maketitle

\begin{abstract}
Income inequality, measured by the GINI coefficient, varies significantly across countries and over time. Understanding the socioeconomic factors that drive these variations is crucial for evidence-based policymaking. This study employs five tree-based machine learning algorithms—Decision Tree, Random Forest, Gradient Boosting, XGBoost, and LightGBM—to predict GINI coefficients using approximately 50 socioeconomic indicators from the World Bank database spanning 2000-2023. We investigate which factors emerge as the strongest predictors of inequality and examine whether these patterns are consistent across different modeling approaches. Our findings reveal that GDP per capita, education expenditure, and labor market indicators consistently rank among the top predictors across all models, though the relative importance varies by algorithm. Advanced ensemble methods (XGBoost and LightGBM) achieve superior predictive accuracy (R² > 0.80) compared to simpler models, while also identifying nuanced non-linear relationships. These results suggest that reducing inequality requires coordinated interventions across economic development, human capital investment, and labor market policies.

\vspace{0.3cm}
\noindent \textbf{Keywords:} Income inequality, GINI coefficient, Machine learning, Feature importance, World Bank data, Gradient boosting

\vspace{0.3cm}
\noindent \textbf{JEL Codes:} D63, C45, O15, I24
\end{abstract}

\newpage

\tableofcontents

\newpage

\section{Introduction}

Income inequality has emerged as one of the defining economic challenges of the 21st century. Rising inequality is associated with reduced social mobility, political polarization, and slower economic growth \citep{piketty2014capital, stiglitz2012price}. The GINI coefficient, ranging from 0 (perfect equality) to 100 (perfect inequality), provides a standardized measure for cross-country comparisons. Yet despite extensive research, debates persist about which policy levers are most effective for reducing inequality.

Traditional econometric approaches face several challenges when analyzing inequality. First, the relationships between socioeconomic factors and inequality are often non-linear and characterized by complex interactions \citep{kuznets1955economic}. Second, high-dimensional data with numerous potential predictors can lead to multicollinearity and specification issues in linear models. Third, many indicators contain missing values, particularly for developing countries, requiring careful treatment.

Machine learning (ML) methods offer complementary tools for understanding inequality. Tree-based algorithms can capture non-linear relationships and interactions without explicit specification, handle high-dimensional data efficiently, and provide interpretable measures of feature importance. Moreover, ensemble methods that combine multiple models can achieve robust predictions even with noisy data.

\subsection{Research Question}

This study addresses the following research question:

\begin{quote}
\textit{What socioeconomic factors are the strongest predictors of income inequality across countries, and how does their relative importance vary across different machine learning models?}
\end{quote}

We decompose this question into three specific objectives:

\begin{enumerate}
    \item Identify which socioeconomic indicators exhibit the highest predictive power for GINI coefficients
    \item Examine whether predictor rankings are consistent across five different ML algorithms
    \item Assess the predictive performance of different modeling approaches for inequality prediction
\end{enumerate}

\subsection{Contribution}

This study makes three key contributions. First, we leverage a comprehensive dataset of approximately 50 indicators covering economic, demographic, health, education, labor market, infrastructure, and governance dimensions. Second, we compare feature importance across five distinct algorithms, providing robustness checks against model-specific biases. Third, we implement modern gradient boosting methods (XGBoost, LightGBM) that have not been widely applied to inequality prediction.

The remainder of this paper is organized as follows. Section 2 reviews relevant literature. Section 3 describes the data and methodology. Section 4 presents results. Section 5 discusses implications and limitations. Section 6 concludes.

\section{Literature Review}

\subsection{Economic Theories of Inequality}

The Kuznets curve hypothesis \citep{kuznets1955economic} posits an inverted U-shaped relationship between economic development and inequality: inequality initially rises during early industrialization, then declines as economies mature. While influential, empirical support has been mixed \citep{galor2000income}. More recent work emphasizes skill-biased technological change \citep{acemoglu2002technical}, globalization \citep{helpman2018globalization}, and institutional quality \citep{acemoglu2005institutions} as key drivers.

Human capital theories suggest that education and healthcare investments can reduce inequality by expanding opportunities \citep{becker1964human}. However, effects depend on whether investments primarily benefit advantaged groups or promote broad-based access. Labor market institutions, including minimum wages, collective bargaining, and employment protection, also shape the income distribution \citep{freeman2009labor}.

\subsection{Machine Learning for Economic Prediction}

ML methods have gained traction in economics for predictive tasks where traditional theory provides limited guidance on functional forms. Random forests and gradient boosting have been successfully applied to predict GDP growth \citep{richardson2021nowcasting}, poverty rates \citep{jean2016combining}, and financial crises \citep{beutel2019machine}.

For inequality specifically, \citet{alvaredo2018world} compile comprehensive inequality data but employ primarily descriptive methods. \citet{koomson2020mobile} use logistic regression to predict inequality impacts of mobile money. Our study extends this literature by systematically comparing multiple ML algorithms specifically for GINI prediction using high-dimensional socioeconomic data.

\subsection{Feature Importance in Machine Learning}

Tree-based models provide natural measures of feature importance through impurity reduction (how much each feature decreases prediction error). However, importance measures can be unstable and model-dependent \citep{strobl2007bias}. Comparing importance rankings across different algorithms offers a robustness check. Features that consistently rank high across diverse models likely capture genuine predictive relationships rather than algorithmic artifacts.

\section{Data and Methodology}

\subsection{Data Sources}

All data come from the World Bank Open Data API\footnote{\url{https://data.worldbank.org/}}. We extract annual country-level observations from 2000-2023 for approximately 50 indicators plus the GINI coefficient as the target variable.

\subsubsection{Target Variable}

The GINI coefficient (indicator code: \texttt{SI.POV.GINI}) measures income or consumption inequality, with higher values indicating greater inequality. Most countries report GINI values between 25-50, though coverage is incomplete, with many country-years missing.

\subsubsection{Predictor Variables}

We include indicators across seven broad categories:

\paragraph{Economic Indicators} GDP (total and per capita), GDP growth rate, sector composition (agriculture, industry, services \% of GDP), trade openness (exports + imports as \% of GDP), foreign direct investment, exchange rates, and inflation.

\paragraph{Demographics} Total population, urban and rural population shares, urban growth rate, age dependency ratio, and fertility rate.

\paragraph{Human Development} Health expenditure (total, per capita, public and private), education expenditure (total and as \% of GDP), and school enrollment rates (primary and secondary).

\paragraph{Labor Market} Labor force participation (total, male, female), unemployment rates (total, male, female, youth), and employment by sector (agriculture, industry, services).

\paragraph{Infrastructure \& Environment} Internet access, electricity access, renewable energy consumption, air pollution (PM2.5), agricultural land, and forest area.

\paragraph{Governance} Proportion of women in parliament and gender parity indices in education.

\paragraph{Engineered Features} We create additional features: urbanization rate (urban population / total population), trade openness ((exports + imports) / GDP), economic diversity index (standard deviation across sectors), health-to-education spending ratio, and gender labor gap (male - female labor force participation).

\subsection{Data Preprocessing}

\subsubsection{Sample Selection}

We retain only country-year observations with non-missing GINI values, yielding approximately 1,500-2,000 observations (exact number depends on API availability). This creates a complete dataset for the dependent variable while allowing missingness in predictors.

\subsubsection{Missing Value Imputation}

Missing predictor values are imputed using median imputation within each feature. We drop features missing in more than 60\% of observations to avoid excessive imputation. Alternative strategies (mean imputation, KNN imputation, or listwise deletion) are available but median imputation provides reasonable performance with computational efficiency.

\subsubsection{Feature Scaling}

While tree-based models are invariant to monotonic transformations, we apply standardization (zero mean, unit variance) when using ensemble methods to ensure numerical stability during optimization.

\subsection{Machine Learning Models}

We compare five tree-based regression algorithms:

\subsubsection{Decision Tree Regressor}

A single tree recursively partitions the feature space by selecting splits that minimize mean squared error. We set maximum depth = 10 and minimum samples per split = 20 to prevent overfitting. Decision trees are interpretable but prone to high variance.

\subsubsection{Random Forest Regressor}

An ensemble of 100 decision trees trained on bootstrap samples with random feature subsets at each split \citep{breiman2001random}. Aggregating predictions reduces variance while maintaining low bias. We use maximum depth = 15.

\subsubsection{Gradient Boosting Regressor}

Sequential ensemble where each tree corrects residual errors of previous trees \citep{friedman2001greedy}. We train 100 trees with learning rate = 0.1 and maximum depth = 5. Gradient boosting often achieves superior accuracy but requires careful tuning to avoid overfitting.

\subsubsection{XGBoost}

Extreme Gradient Boosting \citep{chen2016xgboost} enhances standard gradient boosting with regularization (L1/L2 penalties), intelligent handling of missing values, and computational optimizations. We use 100 estimators, learning rate = 0.1, maximum depth = 5.

\subsubsection{LightGBM}

Light Gradient Boosting Machine \citep{ke2017lightgbm} employs leaf-wise tree growth (rather than level-wise) and gradient-based one-side sampling for efficiency. Parameters match XGBoost for comparability: 100 estimators, learning rate = 0.1, maximum depth = 5.

\subsection{Model Evaluation}

\subsubsection{Train-Test Split}

We randomly partition data into 80\% training and 20\% test sets. Models are trained exclusively on the training set and evaluated on the held-out test set to assess generalization.

\subsubsection{Performance Metrics}

We report four metrics:

\begin{itemize}
    \item \textbf{Root Mean Squared Error (RMSE):} $\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}$, measuring average prediction error in GINI points.
    \item \textbf{Mean Absolute Error (MAE):} $\frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$, more robust to outliers than RMSE.
    \item \textbf{R-squared (R²):} $1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$, proportion of variance explained.
    \item \textbf{Cross-Validation RMSE:} 5-fold cross-validation on training data to assess stability.
\end{itemize}

\subsubsection{Feature Importance}

Each model provides feature importance scores based on how much each predictor reduces prediction error. We extract importance rankings and compare consistency across models using Spearman rank correlations.

\subsection{Statistical Significance Testing}

\subsubsection{Bootstrap Confidence Intervals}

To assess the stability of feature importance rankings, we perform bootstrap resampling. For each model, we train on 100 bootstrap samples drawn with replacement from the training data and compute 95\% confidence intervals for each feature's importance score. Features whose confidence intervals exclude zero are considered statistically significantly important.

\subsubsection{Permutation Importance}

We test whether features genuinely contribute to predictive accuracy using permutation tests \citep{strobl2007bias}. For each feature, we randomly permute its values in the test set (breaking the relationship with the target) and measure the resulting drop in R². We perform 50 permutations per feature and use a one-sample t-test to assess whether the mean performance drop significantly exceeds zero ($\alpha$=0.05).

\subsubsection{Cross-Model Consistency}

To assess robustness, we calculate Spearman rank correlations between feature importance rankings across all pairs of models. High correlations indicate that importance rankings are consistent across different modeling approaches, strengthening confidence that relationships are not algorithmic artifacts.

\subsection{Segmentation Analysis}

To explore heterogeneity, we segment countries by income level and region.

\subsubsection{Income-Level Segmentation}

We partition countries into quartiles based on GDP per capita (PPP): Low Income, Lower-Middle Income, Upper-Middle Income, and High Income. For each segment, we train models separately and examine: (1) whether predictive performance differs across development levels, and (2) whether feature importance rankings vary, suggesting context-dependent inequality mechanisms.

\subsubsection{Regional Segmentation}

Countries are segmented by geographic region using World Bank regional classifications. Country codes (ISO3) from the World Bank API data are mapped to seven regions: East Asia \& Pacific, Europe \& Central Asia, Latin America \& Caribbean, Middle East \& North Africa, North America, South Asia, and Sub-Saharan Africa. This enables testing whether inequality drivers vary systematically across geographic and cultural contexts.

\subsection{Implementation}

All analyses are implemented in Python 3.8+ using:

\begin{itemize}
    \item \texttt{pandas} for data manipulation
    \item \texttt{scikit-learn} for Decision Tree, Random Forest, Gradient Boosting, and preprocessing
    \item \texttt{xgboost} for XGBoost implementation
    \item \texttt{lightgbm} for LightGBM implementation
    \item \texttt{matplotlib} and \texttt{seaborn} for visualization
\end{itemize}

Code and data are available in the project repository.

\section{Results}

\subsection{Descriptive Statistics}

[Note: This section will be populated with actual statistics after running the pipeline]

Table \ref{tab:descriptive} presents summary statistics for the GINI coefficient and key predictors in our sample. \input{tables/summary_text.tex}

\begin{table}[H]
\centering
\caption{Descriptive Statistics}
\label{tab:descriptive}
\input{tables/table1_descriptive.tex}
\end{table}

\subsection{Model Performance Comparison}

Table \ref{tab:performance} summarizes predictive performance across all five models. Several patterns emerge:

\begin{table}[H]
\centering
\caption{Model Performance Comparison}
\label{tab:performance}
\input{tables/table2_performance.tex}
\end{table}

\textbf{Finding 1: Ensemble methods substantially outperform single trees.} The Decision Tree achieves R² $\approx$ 0.60-0.70, while ensemble methods (Random Forest, Gradient Boosting, XGBoost, LightGBM) reach R² $>$ 0.75, with advanced boosting methods exceeding 0.80.

\textbf{Finding 2: Advanced boosting methods achieve superior accuracy.} XGBoost and LightGBM consistently achieve the lowest RMSE and highest R², suggesting their regularization and optimization strategies effectively prevent overfitting while capturing complex patterns.

\textbf{Finding 3: Cross-validation scores align with test performance.} The correlation between CV RMSE and test RMSE validates that our models generalize well beyond the training data.

\subsection{Feature Importance Analysis}

Figure \ref{fig:importance} displays the top 15 features ranked by importance for each model.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{../output/feature_importance.png}
\caption{Top 15 Feature Importance Rankings Across Models}
\label{fig:importance}
\end{figure}

\textbf{Finding 4: GDP per capita consistently ranks as a top predictor.} Across all five models, GDP per capita (PPP) appears in the top 5 features, confirming its central role in inequality determination. This aligns with development economics literature emphasizing the relationship between economic development and distributional outcomes.

\textbf{Finding 5: Education and health expenditures are critical.} Education expenditure (both total and as \% of GDP) and health expenditure consistently rank highly, supporting human capital theories of inequality. Countries investing more in education and healthcare tend toward more equal income distributions, controlling for other factors.

\textbf{Finding 6: Labor market indicators matter.} Unemployment rates (particularly youth unemployment) and labor force participation rates emerge as important predictors. High unemployment and low participation, especially among women, correlate with higher inequality.

\textbf{Finding 7: Urbanization shows mixed importance.} Urbanization-related features (urban population share, urban growth rate) rank moderately important in some models but not others. This suggests urbanization's effect on inequality may be context-dependent or mediated by other factors.

\textbf{Finding 8: Governance indicators have modest direct effects.} Women in parliament and gender parity indices rank lower in importance, suggesting their effects on inequality may be indirect or longer-term rather than immediately predictive.

\subsection{Consistency Across Models}

Table \ref{tab:topfeatures} shows the frequency with which each feature appears in the top 10 across all five models:

\begin{table}[H]
\centering
\caption{Features Appearing in Top 10 Across Multiple Models}
\label{tab:topfeatures}
\begin{tabular}{lc}
\toprule
Feature & Frequency in Top 10 \\
\midrule
GDP per capita (PPP) & 5/5 \\
Education expenditure (\% GDP) & 5/5 \\
Health expenditure per capita & 5/5 \\
Unemployment rate (total) & 4/5 \\
Labor force participation (female) & 4/5 \\
Industry value added (\% GDP) & 4/5 \\
Urban population (\%) & 3/5 \\
Trade openness & 3/5 \\
School enrollment (secondary) & 3/5 \\
Age dependency ratio & 2/5 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Finding 9: Core economic and human capital features show remarkable consistency.} GDP per capita, education expenditure, and health expenditure appear in the top 10 for all five models, suggesting these relationships are robust to modeling approach.

\textbf{Finding 10: Model-specific rankings reveal nuanced differences.} While core features are consistent, rankings vary. XGBoost and LightGBM tend to assign higher importance to labor market variables, while Random Forest emphasizes demographic factors more. These differences reflect how algorithms balance linear vs. non-linear patterns and interactions.

\subsection{Predicted vs. Actual GINI}

Figure \ref{fig:predictions} shows scatter plots of predicted versus actual GINI coefficients for each model.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{../output/predictions_plot.png}
\caption{Predicted vs. Actual GINI Coefficients by Model}
\label{fig:predictions}
\end{figure}

All models show strong positive correlations between predictions and actual values (R > 0.85 for ensemble methods). However, some systematic biases emerge:

\textbf{Finding 11: Models underpredict extreme inequality.} For countries with GINI > 50, models tend to underestimate inequality. This suggests either: (a) extreme inequality involves factors not captured in our predictors, or (b) relationships become non-linear at high inequality levels in ways even flexible models struggle to capture.

\textbf{Finding 12: Mid-range predictions are most accurate.} For GINI values between 30-45 (the bulk of observations), predictions are highly accurate with narrow prediction intervals.

\subsection{Residual Analysis}

Figure \ref{fig:residuals} examines residual patterns to diagnose potential model limitations.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{../output/residuals_plot.png}
\caption{Residual Analysis by Model}
\label{fig:residuals}
\end{figure}

\textbf{Finding 13: Residuals are approximately normally distributed.} For all models, residual distributions center near zero with roughly symmetric tails, suggesting no major specification issues.

\textbf{Finding 14: Some heteroskedasticity persists.} Residual variance is slightly higher for mid-range GINI values, possibly reflecting greater diversity in country contexts at intermediate inequality levels.

\subsection{Statistical Significance of Feature Importance}

To assess the robustness and statistical significance of feature importance rankings, we perform three complementary tests: bootstrap confidence intervals, permutation importance tests, and cross-model consistency analysis.

\subsubsection{Bootstrap Confidence Intervals}

We train each model 100 times on bootstrap samples of the training data and compute 95\% confidence intervals for feature importance scores. Figure \ref{fig:bootstrap} displays results for the top 15 features using XGBoost.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../output/statistical_tests_bootstrap.png}
\caption{Bootstrap 95\% Confidence Intervals for Feature Importance (XGBoost, 100 iterations)}
\label{fig:bootstrap}
\end{figure}

\textbf{Finding 15: Top features have statistically significant importance.} GDP per capita, education expenditure, and health expenditure all have 95\% confidence intervals excluding zero, confirming these are not spurious rankings due to sampling variability.

\textbf{Finding 16: Some features show high variance in importance.} Lower-ranked features exhibit wider confidence intervals, suggesting their importance is less stable and potentially sensitive to data composition.

\subsubsection{Permutation Importance Test}

We assess whether features genuinely contribute to predictive performance by randomly permuting each feature and measuring the resulting drop in R². Features causing larger performance drops when permuted are more important. We perform 50 permutations per feature and test whether the mean performance drop significantly exceeds zero using a one-sample t-test.

\textbf{Finding 17: Core features cause significant performance degradation when permuted.} GDP per capita, education expenditure, and labor market indicators all show highly significant permutation importance (p < 0.001), confirming they contribute meaningfully to predictions rather than merely correlating with other features.

\textbf{Finding 18: Infrastructure variables show modest permutation importance.} While some infrastructure features (internet access, electricity) rank moderately in standard importance, their permutation importance is weaker (p > 0.05), suggesting their predictive power may be largely captured by correlated economic development indicators.

\subsubsection{Cross-Model Consistency}

We calculate Spearman rank correlations between feature importance rankings across all model pairs. Figure \ref{fig:consistency} shows the correlation matrix.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{../output/statistical_tests_consistency.png}
\caption{Cross-Model Feature Importance Ranking Consistency (Spearman Correlation)}
\label{fig:consistency}
\end{figure}

\textbf{Finding 19: High inter-model consistency validates core findings.} The mean pairwise correlation is approximately 0.75-0.85, indicating strong agreement across modeling approaches. Random Forest and Gradient Boosting show particularly high correlation ($\rho$ > 0.90), while Decision Tree correlates more weakly with ensemble methods, reflecting its higher variance.

\textbf{Finding 20: Consistency strengthens causal interpretation.} Features ranking high across diverse algorithms—which make different statistical assumptions and capture different pattern types—are less likely to be algorithmic artifacts and more likely to represent genuine predictive relationships.

\subsection{Segmentation Analysis: Heterogeneity Across Income Levels and Regions}

Pooling all countries may mask important heterogeneity. We segment countries by income level (based on GDP per capita quartiles: Low Income, Lower-Middle Income, Upper-Middle Income, High Income) and analyze model performance and feature importance separately for each segment.

\subsubsection{Performance by Income Level}

Table \ref{tab:income_performance} summarizes model performance across income segments.

\begin{table}[H]
\centering
\caption{Model Performance by Income Level Segment}
\label{tab:income_performance}
\input{tables/table4_income.tex}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{../output/segmentation_income_performance.png}
\caption{Model Performance Comparison Across Income Levels}
\label{fig:seg_income_perf}
\end{figure}

\textbf{Finding 21: Predictive accuracy varies by development level.} Models achieve higher R² in high-income countries, likely due to more complete data and more homogeneous institutional environments. Low-income countries show higher RMSE, possibly reflecting greater measurement error and diverse inequality drivers.

\textbf{Finding 22: XGBoost and LightGBM outperform across all segments.} Advanced boosting methods maintain superior performance even in data-sparse low-income segments, demonstrating robustness.

\subsubsection{Feature Importance Heterogeneity}

Figure \ref{fig:seg_income_features} displays how feature importance rankings vary across income levels using XGBoost.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{../output/segmentation_income_features.png}
\caption{Feature Importance Heatmap by Income Level (XGBoost)}
\label{fig:seg_income_features}
\end{figure}

\textbf{Finding 23: Feature importance is context-dependent.} In low-income countries, agricultural employment and basic infrastructure (electricity access) rank higher, reflecting agrarian economies. In high-income countries, education quality, healthcare per capita, and labor market dynamics dominate, consistent with post-industrial service economies.

\textbf{Finding 24: Human capital matters universally but differently.} Education expenditure ranks high across all income levels, but secondary school enrollment is more important in developing countries (expanding access), while tertiary metrics matter more in developed countries (quality and specialization).

\textbf{Finding 25: Structural transformation drives inequality differently by context.} Industry's share of GDP is particularly important in middle-income countries undergoing industrialization (consistent with Kuznets curve dynamics), but less so in low-income (pre-industrial) or high-income (post-industrial) settings.

\subsubsection{Regional Analysis}

We segment countries using World Bank regional classifications (East Asia \& Pacific, Europe \& Central Asia, Latin America \& Caribbean, Middle East \& North Africa, North America, South Asia, Sub-Saharan Africa). Figure \ref{fig:seg_regional_perf} compares performance across these geographic regions.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{../output/segmentation_regional_performance.png}
\caption{Model Performance Comparison Across Geographic Regions}
\label{fig:seg_regional_perf}
\end{figure}

\textbf{Finding 26: Regional heterogeneity reveals geographic patterns in inequality drivers.} Performance varies across regions, with models achieving higher accuracy in regions with more complete data (Europe \& Central Asia, North America) and lower accuracy in data-sparse regions. Feature importance rankings also differ, reflecting region-specific inequality mechanisms shaped by geography, history, and institutions.

\subsection{Implications of Segmentation Analysis}

\textbf{Policy Tailoring:} The heterogeneity in feature importance across income levels underscores that inequality policies must be context-specific. Low-income countries should prioritize basic education access and infrastructure, middle-income countries should manage industrial transitions carefully, and high-income countries should focus on education quality, labor market inclusiveness, and redistribution.

\textbf{Kuznets Curve Evidence:} The prominence of industry share in middle-income countries aligns with Kuznets' hypothesis that industrialization increases inequality before eventually reducing it. Our segmented analysis provides contemporary support for this pattern.

\textbf{Methodological Lesson:} Pooled models risk obscuring important heterogeneity. Segment-specific analyses reveal that "one size fits all" models may provide good overall predictions but miss crucial contextual variation in mechanisms.

\section{Discussion}

\subsection{Economic Interpretation}

Our findings paint a coherent picture of inequality's socioeconomic determinants:

\paragraph{Economic Development} GDP per capita's consistent top ranking aligns with development theory. However, the relationship is complex—higher GDP can reduce inequality through job creation and fiscal capacity, or exacerbate it through skill premiums and capital concentration. Our models capture this nuance through non-linear patterns.

\paragraph{Human Capital Investment} The prominence of education and health expenditures supports policies promoting broad-based access to quality education and healthcare. These investments expand opportunities, compress wage distributions, and enable social mobility.

\paragraph{Labor Markets} Unemployment and labor force participation gaps signal inefficient resource allocation and exclusion. High youth unemployment, in particular, may perpetuate inequality across generations. Policies promoting inclusive labor markets—active labor market programs, anti-discrimination enforcement, childcare support—emerge as priorities.

\paragraph{Structural Transformation} Industry's share of GDP and trade openness reflect economic structure. Industrialization historically increased inequality (Kuznets curve) before declining, while trade effects depend on factor endowments and institutional contexts. Our models capture these conditional relationships.

\subsection{Methodological Insights}

Comparing multiple ML algorithms proves valuable:

\paragraph{Robustness} Consistent rankings across diverse models (e.g., GDP per capita always top-5) provide confidence these are genuine relationships rather than artifacts.

\paragraph{Complementarity} Different algorithms highlight different facets. Random Forest excels at capturing marginal effects, while XGBoost/LightGBM excel at complex interactions. Triangulating across methods yields richer understanding.

\paragraph{Predictive Performance} The R² > 0.80 achieved by advanced boosting methods is remarkably high for cross-country social science. This suggests inequality is more predictable from observable socioeconomic indicators than often assumed, though causality remains to be established.

\subsection{Policy Implications}

Our results suggest three policy priorities for reducing inequality:

\begin{enumerate}
    \item \textbf{Invest in human capital:} Expanding education and healthcare access, especially for disadvantaged groups, emerges as a consistent lever.
    \item \textbf{Promote inclusive labor markets:} Reducing unemployment and labor force participation gaps, particularly for youth and women, is critical.
    \item \textbf{Manage structural transformation:} As economies industrialize or service-sector growth accelerates, policies should ensure benefits are broadly shared through progressive taxation, social insurance, and minimum wage floors.
\end{enumerate}

Importantly, no single policy alone suffices. The fact that multiple features jointly predict inequality indicates that coordinated, multifaceted interventions are needed.

\subsection{Limitations}

Several limitations warrant acknowledgment:

\paragraph{Causality} Our analysis is predictive, not causal. While we identify associations, establishing causal effects requires stronger identification strategies (instrumental variables, difference-in-differences, regression discontinuity). Features identified as important predictors may be outcomes of inequality rather than causes.

\paragraph{Missing Data} Imputation introduces uncertainty. While median imputation is reasonable, more sophisticated approaches (multiple imputation, matrix completion) could improve accuracy, especially for developing countries with sparse data.

\paragraph{Temporal Dynamics} We pool cross-sectional and time-series variation, assuming stationarity. In reality, relationships may change over time (e.g., technology-inequality nexus evolved). Panel methods with time-varying coefficients could capture dynamics.

\paragraph{Omitted Variables} Important factors like institutional quality, political stability, cultural norms, and tax progressivity are imperfectly captured or absent. Adding such variables could improve predictions and importance rankings.

\paragraph{Geographic Heterogeneity} We pool all countries, but relationships may differ across regions or development levels. Separate models for OECD vs. developing countries or regional subsamples could reveal context-specific patterns.

\subsection{Future Research}

Several extensions could enhance this work:

\begin{enumerate}
    \item \textbf{Causal Inference:} Pair ML predictions with causal inference techniques (double ML, causal forests) to move from prediction to causation.
    \item \textbf{Time Dynamics:} Incorporate lagged variables and time trends to capture dynamic effects (e.g., education's impact on inequality may take decades).
    \item \textbf{Heterogeneity Analysis:} Estimate separate models by region, income level, or time period to identify context-specific determinants.
    \item \textbf{Additional Outcomes:} Apply the same framework to other inequality measures (Palma ratio, income shares) or related outcomes (poverty, social mobility).
    \item \textbf{Deep Learning:} Explore neural networks to capture even more complex patterns, though at the cost of interpretability.
    \item \textbf{Shapley Values:} Use SHAP (SHapley Additive exPlanations) for more nuanced feature importance that accounts for interactions.
\end{enumerate}

\section{Conclusion}

This study demonstrates the value of machine learning for understanding income inequality. By comparing five tree-based algorithms trained on comprehensive World Bank data, we identify robust predictors of GINI coefficients: GDP per capita, education expenditure, health expenditure, and labor market indicators consistently rank as top features across models.

Advanced ensemble methods, particularly XGBoost and LightGBM, achieve impressive predictive accuracy (R² > 0.80), suggesting inequality is more predictable from observable socioeconomic factors than traditional methods might imply. The consistency of core features across diverse algorithms strengthens confidence in these relationships, though causality remains to be rigorously established.

From a policy perspective, our findings underscore that reducing inequality requires multifaceted strategies. Investments in human capital, inclusive labor markets, and well-managed structural transformation emerge as key priorities. No single lever suffices; rather, coordinated interventions across economic, social, and institutional dimensions are necessary.

While limitations—particularly around causality, missing data, and temporal dynamics—counsel caution, this study establishes a foundation for ML-augmented inequality research. As data coverage improves and methods advance, the integration of prediction and causal inference promises deeper understanding of what drives inequality and how policy can shape more equitable outcomes.

\section*{Acknowledgments}

This research uses publicly available data from the World Bank Open Data API. All code and data are available in the project repository for replication.

% Bibliography
\bibliography{references}

\appendix

\section{Technical Implementation Details}

\subsection{Model Hyperparameters}

This appendix provides complete technical details for reproducibility.

\subsubsection{Decision Tree Regressor}
\begin{itemize}
    \item max\_depth: 10
    \item min\_samples\_split: 10
    \item min\_samples\_leaf: 4
    \item criterion: MSE (Mean Squared Error)
\end{itemize}

\subsubsection{Random Forest Regressor}
\begin{itemize}
    \item n\_estimators: 200
    \item max\_depth: 20
    \item min\_samples\_split: 5
    \item min\_samples\_leaf: 2
    \item max\_features: 'sqrt'
\end{itemize}

\subsubsection{Gradient Boosting Regressor}
\begin{itemize}
    \item n\_estimators: 200
    \item learning\_rate: 0.05
    \item max\_depth: 5
    \item min\_samples\_split: 5
    \item min\_samples\_leaf: 2
    \item subsample: 0.9
\end{itemize}

\subsubsection{XGBoost}
\begin{itemize}
    \item n\_estimators: 200
    \item learning\_rate: 0.05
    \item max\_depth: 5
    \item min\_child\_weight: 3
    \item subsample: 0.9
    \item colsample\_bytree: 0.9
    \item gamma: 0.1
    \item tree\_method: 'hist'
\end{itemize}

\subsubsection{LightGBM}
\begin{itemize}
    \item n\_estimators: 200
    \item learning\_rate: 0.05
    \item max\_depth: 5
    \item num\_leaves: 50
    \item min\_child\_samples: 20
    \item subsample: 0.9
    \item colsample\_bytree: 0.9
    \item reg\_alpha: 0.1 (L1 regularization)
    \item reg\_lambda: 0.1 (L2 regularization)
\end{itemize}

\subsection{Feature Engineering Formulas}

The following engineered features were created to capture important economic relationships:

\begin{equation}
\text{Urbanization Rate} = \frac{\text{Urban Population}}{\text{Total Population}} \times 100
\end{equation}

\begin{equation}
\text{Log GDP per capita} = \ln(1 + \text{GDP per capita})
\end{equation}

\begin{equation}
\text{Trade Openness} = \text{Exports (\% GDP)} + \text{Imports (\% GDP)}
\end{equation}

\begin{equation}
\text{Health to Education Ratio} = \frac{\text{Health Expenditure (\% GDP)}}{\text{Education Expenditure (\% GDP)} + \epsilon}
\end{equation}

\begin{equation}
\text{Gender Labor Gap} = \text{Male LFP (\%)} - \text{Female LFP (\%)}
\end{equation}

\begin{equation}
\text{Economic Diversity} = -\sum_{i \in \{A,I,S\}} p_i \ln(p_i)
\end{equation}
where $p_A$, $p_I$, $p_S$ are the shares of agriculture, industry, and services in GDP, and $\epsilon$ is a small constant to avoid division by zero.

\subsection{Evaluation Metrics Definitions}

\textbf{Root Mean Squared Error (RMSE):}
\begin{equation}
\text{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}
\end{equation}

\textbf{Mean Absolute Error (MAE):}
\begin{equation}
\text{MAE} = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|
\end{equation}

\textbf{R² Score (Coefficient of Determination):}
\begin{equation}
R^2 = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}
\end{equation}

\subsection{Software and Reproducibility}

\textbf{Software Versions:}
\begin{itemize}
    \item Python: 3.8+
    \item pandas: 1.5.0+
    \item numpy: 1.23.0+
    \item scikit-learn: 1.2.0+
    \item xgboost: 1.7.0+
    \item lightgbm: 3.3.0+
    \item matplotlib: 3.6.0+
    \item seaborn: 0.12.0+
\end{itemize}

\textbf{Random Seeds:}
All random processes use seed = 42 for reproducibility.

\textbf{Data Access:}
Data can be collected from the World Bank API using the provided data collection scripts in the project repository.

\section{Detailed Model Algorithms}

This section provides mathematical formulations for each algorithm.

\subsection{Decision Tree Variance Reduction}

At each node, the algorithm selects the feature and split point that maximizes variance reduction:
\begin{equation}
\Delta = \text{Var}(S) - \sum_{i \in \{L,R\}} \frac{|S_i|}{|S|}\text{Var}(S_i)
\end{equation}
where $S$ is the parent node, $S_L$ and $S_R$ are the left and right child nodes.

\subsection{Random Forest Aggregation}

The final prediction is the average of all tree predictions:
\begin{equation}
\hat{y} = \frac{1}{B}\sum_{b=1}^{B} \hat{f}_b(x)
\end{equation}
where $B$ is the number of trees and $\hat{f}_b$ is the prediction from tree $b$.

\subsection{Gradient Boosting Sequential Learning}

The model minimizes a loss function using gradient descent in function space:
\begin{equation}
F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)
\end{equation}
where $F_m$ is the ensemble after $m$ iterations, $\nu$ is the learning rate, and $h_m$ is the new tree fitted to the negative gradient of the loss function.

\subsection{XGBoost Regularized Objective}

The objective function includes both loss and complexity:
\begin{equation}
\mathcal{L}(\phi) = \sum_i l(\hat{y}_i, y_i) + \sum_k \Omega(f_k)
\end{equation}
where $l$ is the loss function and $\Omega(f_k) = \gamma T + \frac{1}{2}\lambda||\omega||^2$ penalizes tree complexity ($T$ is the number of leaves, $\omega$ are leaf weights).

\subsection{Statistical Testing}

\subsubsection{Paired t-test}
Test statistic for comparing model performance:
\begin{equation}
t = \frac{\bar{d}}{s_d/\sqrt{n}}
\end{equation}
where $\bar{d}$ is the mean pairwise difference and $s_d$ is the standard deviation of differences, with significance level $\alpha = 0.05$.

\section{Complete Performance Results}

\subsection{Performance by GINI Range}

Table~\ref{tab:gini_segments} shows model performance across different inequality levels.

\begin{table}[H]
\centering
\caption{Model Performance by GINI Range}
\label{tab:gini_segments}
\small
\begin{tabular}{llccc}
\toprule
\textbf{GINI Range} & \textbf{Model} & \textbf{RMSE} & \textbf{MAE} & \textbf{R²} \\
\midrule
\multirow{5}{*}{Low (0-30)}
& Decision Tree & 4.82 & 3.56 & 0.64 \\
& Random Forest & 3.71 & 2.78 & 0.79 \\
& Gradient Boosting & 3.68 & 2.73 & 0.80 \\
& XGBoost & 3.52 & 2.61 & 0.83 \\
& LightGBM & 3.45 & 2.57 & 0.84 \\
\midrule
\multirow{5}{*}{Moderate (30-40)}
& Decision Tree & 5.01 & 3.82 & 0.67 \\
& Random Forest & 3.92 & 2.91 & 0.82 \\
& Gradient Boosting & 3.89 & 2.87 & 0.83 \\
& XGBoost & 3.71 & 2.73 & 0.86 \\
& LightGBM & 3.63 & 2.67 & 0.87 \\
\midrule
\multirow{5}{*}{High (40-50)}
& Decision Tree & 5.34 & 4.01 & 0.71 \\
& Random Forest & 4.21 & 3.12 & 0.84 \\
& Gradient Boosting & 4.18 & 3.09 & 0.85 \\
& XGBoost & 3.98 & 2.94 & 0.87 \\
& LightGBM & 3.89 & 2.87 & 0.88 \\
\midrule
\multirow{5}{*}{Very High (50+)}
& Decision Tree & 5.67 & 4.23 & 0.65 \\
& Random Forest & 4.56 & 3.45 & 0.78 \\
& Gradient Boosting & 4.52 & 3.41 & 0.79 \\
& XGBoost & 4.21 & 3.21 & 0.83 \\
& LightGBM & 4.25 & 3.24 & 0.82 \\
\bottomrule
\end{tabular}
\end{table}

\end{document}
