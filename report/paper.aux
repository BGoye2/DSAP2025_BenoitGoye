\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{piketty2014capital,stiglitz2012price}
\citation{kuznets1955economic}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{kuznets1955economic}
\citation{galor2000income}
\citation{acemoglu2002technical}
\citation{helpman2018globalization}
\citation{acemoglu2005institutions}
\citation{becker1964human}
\citation{freeman2009labor}
\citation{richardson2021nowcasting}
\citation{jean2016combining}
\citation{beutel2019machine}
\citation{alvaredo2018world}
\citation{strobl2007bias}
\citation{breiman2001random}
\citation{friedman2001greedy}
\citation{chen2016xgboost}
\citation{ke2017lightgbm}
\@writefile{toc}{\contentsline {section}{\numberline {2}Literature Review and Research Context}{2}{section.2}\protected@file@percent }
\newlabel{sec:literature}{{2}{2}{Literature Review and Research Context}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology and Algorithmic Complexity}{2}{section.3}\protected@file@percent }
\newlabel{sec:methodology}{{3}{2}{Methodology and Algorithmic Complexity}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementation and Parallel Performance}{3}{section.4}\protected@file@percent }
\newlabel{sec:implementation}{{4}{3}{Implementation and Parallel Performance}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Codebase Maintenance and Software Engineering}{4}{section.5}\protected@file@percent }
\newlabel{sec:maintenance}{{5}{4}{Codebase Maintenance and Software Engineering}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{4}{section.6}\protected@file@percent }
\newlabel{sec:results}{{6}{4}{Results}{section.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Model Performance Comparison}}{4}{table.1}\protected@file@percent }
\newlabel{tab:performance}{{1}{4}{Model Performance Comparison}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Top 10 Features by Importance (XGBoost)}}{5}{table.2}\protected@file@percent }
\newlabel{tab:features}{{2}{5}{Top 10 Features by Importance (XGBoost)}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Feature importance rankings across all five models, showing top 15 features for each algorithm. Rural electricity access consistently dominates across all models, while trade openness, gender labor gaps, and economic structure variables appear in most top-10 rankings.}}{5}{figure.1}\protected@file@percent }
\newlabel{fig:importance}{{1}{5}{Feature importance rankings across all five models, showing top 15 features for each algorithm. Rural electricity access consistently dominates across all models, while trade openness, gender labor gaps, and economic structure variables appear in most top-10 rankings}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Bootstrap 95\% confidence intervals for feature importance (XGBoost, 100 iterations). Top features show narrow confidence intervals well above zero, confirming statistical significance, while lower-ranked features exhibit wider intervals indicating less stable importance estimates.}}{5}{figure.2}\protected@file@percent }
\newlabel{fig:bootstrap}{{2}{5}{Bootstrap 95\% confidence intervals for feature importance (XGBoost, 100 iterations). Top features show narrow confidence intervals well above zero, confirming statistical significance, while lower-ranked features exhibit wider intervals indicating less stable importance estimates}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Cross-model feature importance ranking consistency matrix (Spearman correlations). Gradient Boosting and XGBoost show highest consistency ($\rho = 0.83$), while Decision Tree shows lower correlation with ensemble methods, reflecting its higher variance.}}{5}{figure.3}\protected@file@percent }
\newlabel{fig:consistency}{{3}{5}{Cross-model feature importance ranking consistency matrix (Spearman correlations). Gradient Boosting and XGBoost show highest consistency ($\rho = 0.83$), while Decision Tree shows lower correlation with ensemble methods, reflecting its higher variance}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Predicted versus actual GINI coefficients for best-performing model (Gradient Boosting). Points cluster tightly around the 45-degree line (perfect prediction) for GINI values 20--45, demonstrating strong predictive accuracy. Underprediction of extreme inequality (GINI $>$ 50) visible in upper-right region.}}{6}{figure.4}\protected@file@percent }
\newlabel{fig:predictions}{{4}{6}{Predicted versus actual GINI coefficients for best-performing model (Gradient Boosting). Points cluster tightly around the 45-degree line (perfect prediction) for GINI values 20--45, demonstrating strong predictive accuracy. Underprediction of extreme inequality (GINI $>$ 50) visible in upper-right region}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Residual analysis for Gradient Boosting model. Left panel shows residuals versus predicted values, revealing approximate homoskedasticity with slight increase in variance at mid-range predictions. Right panel shows residual distribution approximating normality (mean near zero, symmetric tails), confirming model adequacy.}}{6}{figure.5}\protected@file@percent }
\newlabel{fig:residuals}{{5}{6}{Residual analysis for Gradient Boosting model. Left panel shows residuals versus predicted values, revealing approximate homoskedasticity with slight increase in variance at mid-range predictions. Right panel shows residual distribution approximating normality (mean near zero, symmetric tails), confirming model adequacy}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comprehensive model performance comparison across multiple metrics. Top panels show prediction accuracy (RMSE, MAE, $R^2$), with ensemble methods substantially outperforming single Decision Tree. Bottom panels show training time and memory usage, illustrating efficiency-accuracy tradeoffs. LightGBM achieves best speed-accuracy balance.}}{6}{figure.6}\protected@file@percent }
\newlabel{fig:comparison}{{6}{6}{Comprehensive model performance comparison across multiple metrics. Top panels show prediction accuracy (RMSE, MAE, $R^2$), with ensemble methods substantially outperforming single Decision Tree. Bottom panels show training time and memory usage, illustrating efficiency-accuracy tradeoffs. LightGBM achieves best speed-accuracy balance}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Model performance by income quartile. High-income countries show superior prediction accuracy ($R^2 > 0.90$) due to complete data and institutional homogeneity, while low-income countries exhibit lower accuracy ($R^2 \approx 0.72$) reflecting measurement challenges and diverse inequality drivers. Middle-income countries show intermediate performance, consistent with Kuznets curve transitions.}}{7}{figure.7}\protected@file@percent }
\newlabel{fig:segmentation}{{7}{7}{Model performance by income quartile. High-income countries show superior prediction accuracy ($R^2 > 0.90$) due to complete data and institutional homogeneity, while low-income countries exhibit lower accuracy ($R^2 \approx 0.72$) reflecting measurement challenges and diverse inequality drivers. Middle-income countries show intermediate performance, consistent with Kuznets curve transitions}{figure.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion and Future Directions}{7}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{9}{section.8}\protected@file@percent }
\newlabel{sec:conclusion}{{8}{9}{Conclusion}{section.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Algorithmic Details}{10}{appendix.A}\protected@file@percent }
\bibdata{references}
\bibcite{acemoglu2002technical}{{1}{2002}{{Acemoglu}}{{}}}
\bibcite{acemoglu2005institutions}{{2}{2005}{{Acemoglu and Robinson}}{{}}}
\bibcite{alvaredo2018world}{{3}{2018}{{Alvaredo et~al.}}{{Alvaredo, Chancel, Piketty, Saez, and Zucman}}}
\bibcite{becker1964human}{{4}{1964}{{Becker}}{{}}}
\bibcite{beutel2019machine}{{5}{2019}{{Beutel et~al.}}{{Beutel, List, and von Schweinitz}}}
\bibcite{breiman2001random}{{6}{2001}{{Breiman}}{{}}}
\bibcite{chen2016xgboost}{{7}{2016}{{Chen and Guestrin}}{{}}}
\bibcite{freeman2009labor}{{8}{2010}{{Freeman}}{{}}}
\bibcite{friedman2001greedy}{{9}{2001}{{Friedman}}{{}}}
\bibcite{galor2000income}{{10}{1993}{{Galor and Zeira}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Gradient Boosting Algorithm}{11}{subsection.A.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient Boosting for Regression}}{11}{algorithm.1}\protected@file@percent }
\newlabel{alg:gradboost}{{1}{11}{Gradient Boosting Algorithm}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}XGBoost Regularized Objective}{11}{subsection.A.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}LightGBM's Gradient-Based One-Side Sampling}{11}{subsection.A.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Gradient-Based One-Side Sampling}}{11}{algorithm.2}\protected@file@percent }
\newlabel{alg:goss}{{2}{11}{LightGBM's Gradient-Based One-Side Sampling}{algorithm.2}{}}
\bibcite{helpman2018globalization}{{11}{2018}{{Helpman}}{{}}}
\bibcite{jean2016combining}{{12}{2016}{{Jean et~al.}}{{Jean, Burke, Xie, Davis, Lobell, and Ermon}}}
\bibcite{ke2017lightgbm}{{13}{2017}{{Ke et~al.}}{{Ke, Meng, Finley, Wang, Chen, Ma, Ye, and Liu}}}
\bibcite{kuznets1955economic}{{14}{1955}{{Kuznets}}{{}}}
\bibcite{piketty2014capital}{{15}{2014}{{Piketty}}{{}}}
\bibcite{richardson2021nowcasting}{{16}{2021}{{Richardson et~al.}}{{Richardson, Mulder, and Vehbi}}}
\bibcite{stiglitz2012price}{{17}{2012}{{Stiglitz}}{{}}}
\bibcite{strobl2007bias}{{18}{2007}{{Strobl et~al.}}{{Strobl, Boulesteix, Zeileis, and Hothorn}}}
\gdef \@abspage@last{12}
