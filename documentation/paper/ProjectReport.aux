\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{piketty2014capital,stiglitz2012price}
\citation{kuznets1955economic}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{mullainathan2017machine}
\citation{richardson2021nowcasting}
\citation{jean2016combining}
\citation{beutel2019machine}
\citation{lundberg2017unified}
\citation{acemoglu2002technical}
\citation{helpman2018globalization}
\citation{acemoglu2005institutions}
\citation{kuznets1955economic}
\citation{galor2000income}
\citation{freeman2009labor}
\citation{lee2018human}
\citation{chakravorty2014does}
\citation{zhang2024urban}
\@writefile{toc}{\contentsline {section}{\numberline {2}Literature Review and Research Question}{2}{section.2}\protected@file@percent }
\newlabel{sec:literature}{{2}{2}{Literature Review and Research Question}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{2}{section.3}\protected@file@percent }
\newlabel{sec:methodology}{{3}{2}{Methodology}{section.3}{}}
\citation{breiman1984classification}
\citation{breiman2001random}
\citation{friedman2001greedy}
\citation{chen2016xgboost}
\citation{ke2017lightgbm}
\citation{spearman1904proof}
\citation{wei2022feature,zeng2021feature}
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementation and Parallel Performance}{5}{section.4}\protected@file@percent }
\newlabel{sec:implementation}{{4}{5}{Implementation and Parallel Performance}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Codebase Maintenance}{5}{section.5}\protected@file@percent }
\newlabel{sec:maintenance}{{5}{5}{Codebase Maintenance}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{6}{section.6}\protected@file@percent }
\newlabel{sec:results}{{6}{6}{Results}{section.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Model Performance Comparison}}{6}{table.1}\protected@file@percent }
\newlabel{tab:performance}{{1}{6}{Model Performance Comparison}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Top 10 Features by Importance (XGBoost)}}{6}{table.2}\protected@file@percent }
\newlabel{tab:features}{{2}{6}{Top 10 Features by Importance (XGBoost)}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \small  Feature importance rankings across all five models. Rural electricity access dominates consistently, followed by trade openness and economic structure variables.}}{6}{figure.1}\protected@file@percent }
\newlabel{fig:importance}{{1}{6}{\small Feature importance rankings across all five models. Rural electricity access dominates consistently, followed by trade openness and economic structure variables}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \small  Bootstrap 95\% confidence intervals for feature importance (XGBoost, 100 iterations). Top features show narrow intervals well above zero.}}{7}{figure.2}\protected@file@percent }
\newlabel{fig:bootstrap}{{2}{7}{\small Bootstrap 95\% confidence intervals for feature importance (XGBoost, 100 iterations). Top features show narrow intervals well above zero}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \small  Cross-model consistency matrix (Spearman correlations). Gradient Boosting and XGBoost show highest consistency ($\rho = 0.84$).}}{7}{figure.3}\protected@file@percent }
\newlabel{fig:consistency}{{3}{7}{\small Cross-model consistency matrix (Spearman correlations). Gradient Boosting and XGBoost show highest consistency ($\rho = 0.84$)}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \small  Predicted vs. actual GINI coefficients (Gradient Boosting). Strong accuracy for GINI 20--45; underprediction of extreme inequality ($>$50).}}{7}{figure.4}\protected@file@percent }
\newlabel{fig:predictions}{{4}{7}{\small Predicted vs. actual GINI coefficients (Gradient Boosting). Strong accuracy for GINI 20--45; underprediction of extreme inequality ($>$50)}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \small  Residual analysis for Gradient Boosting. Left: residuals vs. predicted values; Right: residual distribution approximating normality.}}{7}{figure.5}\protected@file@percent }
\newlabel{fig:residuals}{{5}{7}{\small Residual analysis for Gradient Boosting. Left: residuals vs. predicted values; Right: residual distribution approximating normality}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \small  Model performance by income quartile. High-income: $R^2 > 0.90$; low-income: $R^2 \approx 0.72$, reflecting data quality differences.}}{8}{figure.6}\protected@file@percent }
\newlabel{fig:segmentation}{{6}{8}{\small Model performance by income quartile. High-income: $R^2 > 0.90$; low-income: $R^2 \approx 0.72$, reflecting data quality differences}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion and Future Directions}{8}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{9}{section.8}\protected@file@percent }
\newlabel{sec:conclusion}{{8}{9}{Conclusion}{section.8}{}}
\bibdata{references}
\bibcite{acemoglu2002technical}{{1}{2002}{{Acemoglu}}{{}}}
\bibcite{acemoglu2005institutions}{{2}{2005}{{Acemoglu and Robinson}}{{}}}
\bibcite{beutel2019machine}{{3}{2019}{{Beutel et~al.}}{{Beutel, List, and von Schweinitz}}}
\bibcite{breiman2001random}{{4}{2001}{{Breiman}}{{}}}
\bibcite{breiman1984classification}{{5}{1984}{{Breiman et~al.}}{{Breiman, Friedman, Stone, and Olshen}}}
\bibcite{chakravorty2014does}{{6}{2014}{{Chakravorty et~al.}}{{Chakravorty, Pelli, and Marchand}}}
\bibcite{chen2016xgboost}{{7}{2016}{{Chen and Guestrin}}{{}}}
\bibcite{freeman2009labor}{{8}{2010}{{Freeman}}{{}}}
\bibcite{friedman2001greedy}{{9}{2001}{{Friedman}}{{}}}
\bibcite{galor2000income}{{10}{1993}{{Galor and Zeira}}{{}}}
\bibcite{helpman2018globalization}{{11}{2018}{{Helpman}}{{}}}
\bibcite{jean2016combining}{{12}{2016}{{Jean et~al.}}{{Jean, Burke, Xie, Davis, Lobell, and Ermon}}}
\bibcite{ke2017lightgbm}{{13}{2017}{{Ke et~al.}}{{Ke, Meng, Finley, Wang, Chen, Ma, Ye, and Liu}}}
\bibcite{kuznets1955economic}{{14}{1955}{{Kuznets}}{{}}}
\bibcite{lee2018human}{{15}{2018}{{Lee}}{{}}}
\bibcite{lundberg2017unified}{{16}{2017}{{Lundberg and Lee}}{{}}}
\bibcite{mullainathan2017machine}{{17}{2017}{{Mullainathan and Spiess}}{{}}}
\bibcite{piketty2014capital}{{18}{2014}{{Piketty}}{{}}}
\bibcite{richardson2021nowcasting}{{19}{2021}{{Richardson et~al.}}{{Richardson, Mulder, and Vehbi}}}
\bibcite{spearman1904proof}{{20}{1904}{{Spearman}}{{}}}
\bibcite{stiglitz2012price}{{21}{2012}{{Stiglitz}}{{}}}
\bibcite{wei2022feature}{{22}{2022}{{Wei et~al.}}{{Wei, Lu, and Song}}}
\bibcite{zeng2021feature}{{23}{2021}{{Zeng et~al.}}{{Zeng, Wang, Luo, Kang, Tang, Lightstone, Fang, Cornell, Nussinov, and Cheng}}}
\bibcite{zhang2024urban}{{24}{2024}{{Zhang et~al.}}{{Zhang, Li, Liu, Xu, Wang, and Li}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Algorithmic Details}{12}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Gradient Boosting Algorithm}{12}{subsection.A.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient Boosting for Regression (Least Squares)}}{12}{algorithm.1}\protected@file@percent }
\newlabel{alg:gradboost}{{1}{12}{Gradient Boosting Algorithm}{algorithm.1}{}}
\citation{chen2016xgboost}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}XGBoost Regularized Objective}{13}{subsection.A.2}\protected@file@percent }
\citation{ke2017lightgbm}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}LightGBM's Gradient-Based One-Side Sampling}{14}{subsection.A.3}\protected@file@percent }
\citation{ke2017lightgbm}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Gradient-Based One-Side Sampling}}{15}{algorithm.2}\protected@file@percent }
\newlabel{alg:goss}{{2}{15}{LightGBM's Gradient-Based One-Side Sampling}{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Comprehensive Model Performance Comparison}{16}{appendix.B}\protected@file@percent }
\newlabel{app:comparison}{{B}{16}{Comprehensive Model Performance Comparison}{appendix.B}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \small  Comprehensive model performance comparison. Top: accuracy metrics (RMSE, MAE, $R^2$); Bottom: training time and memory. LightGBM achieves best speed-accuracy balance.}}{16}{figure.7}\protected@file@percent }
\newlabel{fig:comparison}{{7}{16}{\small Comprehensive model performance comparison. Top: accuracy metrics (RMSE, MAE, $R^2$); Bottom: training time and memory. LightGBM achieves best speed-accuracy balance}{figure.7}{}}
\gdef \@abspage@last{16}
